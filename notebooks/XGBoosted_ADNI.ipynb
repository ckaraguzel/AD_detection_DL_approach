{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(0)\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "# Create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=30720, out_features=65, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=65, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simpler model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(65, 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*240, 65)  \n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n",
    "        self.fc3 = nn.Linear(65, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 65, 256, 240)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = x.view(x.size(0), -1)  # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# Initialize the network and print its architecture\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 3.0144\n",
      "Epoch 2, training loss: 2.0268\n",
      "Epoch 3, training loss: 1.3780\n",
      "Epoch 4, training loss: 1.0252\n",
      "Epoch 5, training loss: 0.8956\n",
      "Epoch 6, training loss: 0.8356\n",
      "Epoch 7, training loss: 0.8230\n",
      "Epoch 8, training loss: 0.8027\n",
      "Epoch 9, training loss: 0.8040\n",
      "Epoch 10, training loss: 0.8018\n",
      "Epoch 11, training loss: 0.7677\n",
      "Epoch 12, training loss: 0.7583\n",
      "Epoch 13, training loss: 0.7618\n",
      "Epoch 14, training loss: 0.7770\n",
      "Epoch 15, training loss: 0.7990\n",
      "Epoch 1, training loss: 0.7308\n",
      "Epoch 2, training loss: 0.6901\n",
      "Epoch 3, training loss: 0.6770\n",
      "Epoch 4, training loss: 0.6147\n",
      "Epoch 5, training loss: 0.5677\n",
      "Epoch 6, training loss: 0.4948\n",
      "Epoch 7, training loss: 0.4114\n",
      "Epoch 8, training loss: 0.3996\n",
      "Epoch 9, training loss: 0.3288\n",
      "Epoch 10, training loss: 0.2399\n",
      "Epoch 11, training loss: 0.2265\n",
      "Epoch 12, training loss: 0.1683\n",
      "Epoch 13, training loss: 0.1567\n",
      "Epoch 14, training loss: 0.1151\n",
      "Epoch 15, training loss: 0.1041\n",
      "Epoch 16, training loss: 0.1024\n",
      "Epoch 17, training loss: 0.0746\n",
      "Epoch 18, training loss: 0.0625\n",
      "Epoch 19, training loss: 0.0756\n",
      "Epoch 20, training loss: 0.0553\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([0.5,1.0]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Initialize empty lists to store losses\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        l1_lambda = 0.0005\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss+=l1_lambda*l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Compute training loss\n",
    "        train_losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "# Now without regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Compute training loss\n",
    "        train_losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = original_model.conv1\n",
    "        self.fc1 = original_model.fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 65, 256, 240)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = x.view(x.size(0), -1)  # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# Create the new model\n",
    "new_model = MyModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = [] #stores one feature/value for each image for each patient.  Shape is (*,32,65).\n",
    "labels_train = []\n",
    "for data in train_dataloader:\n",
    "    input_data = data[0].float()\n",
    "    labels_train.append(data[1])\n",
    "    output = new_model(input_data.to(device))\n",
    "    outputs.append(output.detach().cpu().numpy()) \n",
    "flattened_outputs = np.concatenate(outputs, axis=0)\n",
    "flattened_labels = np.concatenate(labels_train, axis = 0)\n",
    "del outputs, output\n",
    "np.save('modified_train.npy', flattened_outputs)\n",
    "np.save('modified_labels.npy', flattened_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "outputs_2 = []\n",
    "# Assuming x_test is a PyTorch tensor\n",
    "testloader = torch.utils.data.DataLoader(X_test, batch_size=32)\n",
    "del X_test\n",
    "for data in testloader:\n",
    "    input_data = data.float()\n",
    "    output = new_model(input_data.to(device))\n",
    "    outputs_2.append(output.detach().cpu().numpy()) \n",
    "\n",
    "\n",
    "flattened_outputs_2 = np.concatenate(outputs_2, axis=0)\n",
    "del outputs_2, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('modified_test.npy',flattened_outputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.load('modified_train.npy')\n",
    "y_train = np.load('modified_labels.npy')\n",
    "x_test = np.load('modified_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.67461\n",
      "[1]\tvalidation_0-logloss:0.65708\n",
      "[2]\tvalidation_0-logloss:0.64003\n",
      "[3]\tvalidation_0-logloss:0.62418\n",
      "[4]\tvalidation_0-logloss:0.60934\n",
      "[5]\tvalidation_0-logloss:0.59588\n",
      "[6]\tvalidation_0-logloss:0.58219\n",
      "[7]\tvalidation_0-logloss:0.56986\n",
      "[8]\tvalidation_0-logloss:0.55823\n",
      "[9]\tvalidation_0-logloss:0.54709\n",
      "[10]\tvalidation_0-logloss:0.53690\n",
      "[11]\tvalidation_0-logloss:0.52668\n",
      "[12]\tvalidation_0-logloss:0.51691\n",
      "[13]\tvalidation_0-logloss:0.50839\n",
      "[14]\tvalidation_0-logloss:0.49958\n",
      "[15]\tvalidation_0-logloss:0.49177\n",
      "[16]\tvalidation_0-logloss:0.48348\n",
      "[17]\tvalidation_0-logloss:0.47648\n",
      "[18]\tvalidation_0-logloss:0.46921\n",
      "[19]\tvalidation_0-logloss:0.46211\n",
      "[20]\tvalidation_0-logloss:0.45443\n",
      "[21]\tvalidation_0-logloss:0.44746\n",
      "[22]\tvalidation_0-logloss:0.44075\n",
      "[23]\tvalidation_0-logloss:0.43436\n",
      "[24]\tvalidation_0-logloss:0.42831\n",
      "[25]\tvalidation_0-logloss:0.42232\n",
      "[26]\tvalidation_0-logloss:0.41582\n",
      "[27]\tvalidation_0-logloss:0.41040\n",
      "[28]\tvalidation_0-logloss:0.40510\n",
      "[29]\tvalidation_0-logloss:0.40053\n",
      "[30]\tvalidation_0-logloss:0.39564\n",
      "[31]\tvalidation_0-logloss:0.39128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thetu\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32]\tvalidation_0-logloss:0.38738\n",
      "[33]\tvalidation_0-logloss:0.38274\n",
      "[34]\tvalidation_0-logloss:0.37897\n",
      "[35]\tvalidation_0-logloss:0.37533\n",
      "[36]\tvalidation_0-logloss:0.37138\n",
      "[37]\tvalidation_0-logloss:0.36794\n",
      "[38]\tvalidation_0-logloss:0.36499\n",
      "[39]\tvalidation_0-logloss:0.36183\n",
      "[40]\tvalidation_0-logloss:0.35941\n",
      "[41]\tvalidation_0-logloss:0.35654\n",
      "[42]\tvalidation_0-logloss:0.35302\n",
      "[43]\tvalidation_0-logloss:0.35059\n",
      "[44]\tvalidation_0-logloss:0.34819\n",
      "[45]\tvalidation_0-logloss:0.34592\n",
      "[46]\tvalidation_0-logloss:0.34379\n",
      "[47]\tvalidation_0-logloss:0.34179\n",
      "[48]\tvalidation_0-logloss:0.33950\n",
      "[49]\tvalidation_0-logloss:0.33737\n",
      "[50]\tvalidation_0-logloss:0.33560\n",
      "[51]\tvalidation_0-logloss:0.33373\n",
      "[52]\tvalidation_0-logloss:0.33221\n",
      "[53]\tvalidation_0-logloss:0.33085\n",
      "[54]\tvalidation_0-logloss:0.32906\n",
      "[55]\tvalidation_0-logloss:0.32779\n",
      "[56]\tvalidation_0-logloss:0.32682\n",
      "[57]\tvalidation_0-logloss:0.32583\n",
      "[58]\tvalidation_0-logloss:0.32435\n",
      "[59]\tvalidation_0-logloss:0.32317\n",
      "[60]\tvalidation_0-logloss:0.32195\n",
      "[61]\tvalidation_0-logloss:0.32128\n",
      "[62]\tvalidation_0-logloss:0.32028\n",
      "[63]\tvalidation_0-logloss:0.31855\n",
      "[64]\tvalidation_0-logloss:0.31761\n",
      "[65]\tvalidation_0-logloss:0.31597\n",
      "[66]\tvalidation_0-logloss:0.31506\n",
      "[67]\tvalidation_0-logloss:0.31340\n",
      "[68]\tvalidation_0-logloss:0.31279\n",
      "[69]\tvalidation_0-logloss:0.31262\n",
      "[70]\tvalidation_0-logloss:0.31126\n",
      "[71]\tvalidation_0-logloss:0.31087\n",
      "[72]\tvalidation_0-logloss:0.30960\n",
      "[73]\tvalidation_0-logloss:0.30839\n",
      "[74]\tvalidation_0-logloss:0.30836\n",
      "[75]\tvalidation_0-logloss:0.30753\n",
      "[76]\tvalidation_0-logloss:0.30648\n",
      "[77]\tvalidation_0-logloss:0.30633\n",
      "[78]\tvalidation_0-logloss:0.30554\n",
      "[79]\tvalidation_0-logloss:0.30480\n",
      "[80]\tvalidation_0-logloss:0.30486\n",
      "[81]\tvalidation_0-logloss:0.30437\n",
      "[82]\tvalidation_0-logloss:0.30375\n",
      "[83]\tvalidation_0-logloss:0.30318\n",
      "[84]\tvalidation_0-logloss:0.30302\n",
      "[85]\tvalidation_0-logloss:0.30233\n",
      "[86]\tvalidation_0-logloss:0.30245\n",
      "[87]\tvalidation_0-logloss:0.30193\n",
      "[88]\tvalidation_0-logloss:0.30164\n",
      "[89]\tvalidation_0-logloss:0.30167\n",
      "[90]\tvalidation_0-logloss:0.30111\n",
      "[91]\tvalidation_0-logloss:0.30083\n",
      "[92]\tvalidation_0-logloss:0.30032\n",
      "[93]\tvalidation_0-logloss:0.30076\n",
      "[94]\tvalidation_0-logloss:0.30027\n",
      "[95]\tvalidation_0-logloss:0.30051\n",
      "[96]\tvalidation_0-logloss:0.29983\n",
      "[97]\tvalidation_0-logloss:0.30001\n",
      "[98]\tvalidation_0-logloss:0.30030\n",
      "[99]\tvalidation_0-logloss:0.29971\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Suppose 'data' is your 2D numpy array, with each row a datapoint\n",
    "# And 'labels' is a 1D numpy array containing the class label for each datapoint\n",
    "data = np.vstack((x_train,x_test))  # Your data\n",
    "labels = np.hstack((y_train,y_test))  # Your labels\n",
    "\n",
    "# Split the data into training and testing sets first\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "scale_pos_weight = sum(labels_train == 0) / sum(labels_train == 1)\n",
    "\n",
    "# Define the XGBoost model\n",
    "model = XGBClassifier(\n",
    "    max_depth=5,  # Maximum depth of the trees\n",
    "    learning_rate=0.03,  # Learning rate (eta)\n",
    "    n_estimators=100,  # Number of training rounds\n",
    "    objective='binary:logistic',  # Objective function for binary classification\n",
    "    scale_pos_weight = scale_pos_weight,\n",
    "    random_state=42  # Random seed\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "eval_set = [(data_test, labels_test)]  # Validation set for early stopping\n",
    "model.fit(data_train, labels_train,eval_metric=\"logloss\", eval_set=eval_set)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_prob = model.predict_proba(data_test)\n",
    "\n",
    "# 'preds_prob' now contains the predicted probabilities of the positive class for each datapoint in the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.22%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "def find_optimal_threshold(predictions, y_test):\n",
    "    min_sum = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    # Iterate over possible thresholds from 0 to 1\n",
    "    for threshold in np.arange(0.0, 1, 0.001):\n",
    "        # Apply threshold\n",
    "        preds = (np.array(predictions)[:,1] > threshold).astype(int)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "        # Compute sum of off-diagonal elements\n",
    "        off_diagonal_sum = 164 - np.trace(cm)\n",
    "        #print(cm)\n",
    "        # Update optimal threshold if this threshold is better\n",
    "        if off_diagonal_sum < min_sum and cm[1][1]/np.sum(cm[1]) >0.5:\n",
    "            min_sum = off_diagonal_sum\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(preds_prob, labels_test)\n",
    "preds = (preds_prob[:, 1]> threshold).astype(int)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(labels_test, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042857142857144"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(labels_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91,  7],\n",
       "       [ 6, 44]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(labels_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8209069,
     "datasetId": 4773921,
     "sourceId": 8092331,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
