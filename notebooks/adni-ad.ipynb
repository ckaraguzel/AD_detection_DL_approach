{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8092331,"datasetId":4773921,"databundleVersionId":8209069}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T14:27:09.387207Z","iopub.execute_input":"2024-04-11T14:27:09.387772Z","iopub.status.idle":"2024-04-11T14:27:09.392869Z","shell.execute_reply.started":"2024-04-11T14:27:09.387739Z","shell.execute_reply":"2024-04-11T14:27:09.392001Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/adni-sorted/pixels_CN_1.pkl', 'rb') as f:\n    cn_1 = pickle.load(f)\nwith open('/kaggle/input/adni-sorted/pixels_CN_2.pkl', 'rb') as f:\n    cn_2 = pickle.load(f)\nwith open('/kaggle/input/adni-sorted/pixels_MCI.pkl', 'rb') as f:\n    mci = pickle.load(f)\n    \ncn_1.extend(cn_2)\n\nmci.pop(70)\ndel cn_2\nprint('dataset loaded')\n## Change the shape of patient 2, slice 6 in MCI","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:27:11.695122Z","iopub.execute_input":"2024-04-11T14:27:11.695431Z","iopub.status.idle":"2024-04-11T14:28:00.313167Z","shell.execute_reply.started":"2024-04-11T14:27:11.695408Z","shell.execute_reply":"2024-04-11T14:28:00.312219Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"dataset loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:12:17.858540Z","iopub.execute_input":"2024-04-10T18:12:17.859384Z","iopub.status.idle":"2024-04-10T18:12:17.863296Z","shell.execute_reply.started":"2024-04-10T18:12:17.859342Z","shell.execute_reply":"2024-04-10T18:12:17.862249Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Set the seed\nrandom.seed(42)\n# Calculate the number of samples\nl_cn = len(cn_1)\nl_mci = len(mci)\nnum_samples_1 = int(0.8 * l_cn)\nnum_samples_2 = int(0.8* l_mci)\n\ntrain_indices_1 = random.sample(list(range(l_cn)), num_samples_1)\ntrain_indices_2 = random.sample(list(range(l_mci)), num_samples_2)\n\ntest_indices_1 = list(set(list(range(l_cn))) - set(train_indices_1))\ntest_indices_2 = list(set(list(range(l_mci))) - set(train_indices_2))\n\ndef scale(arr):\n    return (arr - np.min(arr))/(np.max(arr) - np.min(arr))\n\n# # Sample 80% of the entries\n# X_train_1 = [np.array([scale(cn_1[i][j]) for j in range(65)]) for i in train_indices_1]\n# np.save('X_train_1.npy',np.array(X_train_1))\n# X_train_2 = [np.array([scale(mci[i][j]) for j in range(65)]) for i in train_indices_2]\n# np.save('X_train_2.npy', np.array(X_train_2))\n\n# del X_train_1, X_train_2, train_indices_1, train_indices_2\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T14:28:00.315054Z","iopub.execute_input":"2024-04-11T14:28:00.315483Z","iopub.status.idle":"2024-04-11T14:28:00.323878Z","shell.execute_reply.started":"2024-04-11T14:28:00.315448Z","shell.execute_reply":"2024-04-11T14:28:00.322886Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_test_1 = [np.array([scale(cn_1[i][j]) for j in range(65)]) for i in test_indices_1]\nnp.save('X_test_1.npy',np.array(X_test_1))\nX_test_2 = [np.array([scale(mci[i][j]) for j in range(65)]) for i in test_indices_2]\nnp.save('X_test_2.npy',np.array(X_test_2))\n\ndel X_test_1, X_test_2, test_indices_1, test_indices_2\n#del mci, cn_1","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:28:00.324927Z","iopub.execute_input":"2024-04-11T14:28:00.325176Z","iopub.status.idle":"2024-04-11T14:28:08.768003Z","shell.execute_reply.started":"2024-04-11T14:28:00.325154Z","shell.execute_reply":"2024-04-11T14:28:08.767100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_1 = np.load('/kaggle/input/x-train-1-2/X_train_1.npy')\nX_train_2 = np.load('/kaggle/input/x-train-1-2/X_train_2.npy')\n\n# Create labels for your data\ny_train_1 = np.zeros(len(X_train_1))  # Class 0 for X_train_1\ny_train_2 = np.ones(len(X_train_2))   # Class 1 for X_train_2\n\n# Stack your data and labels\nX_train = np.vstack((X_train_1, X_train_2))\ny_train = np.hstack((y_train_1, y_train_2))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:18:39.752475Z","iopub.execute_input":"2024-04-11T04:18:39.753000Z","iopub.status.idle":"2024-04-11T04:21:01.565502Z","shell.execute_reply.started":"2024-04-11T04:18:39.752960Z","shell.execute_reply":"2024-04-11T04:21:01.564430Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"del X_train_1, X_train_2, y_train_1, y_train_2","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:21:01.567131Z","iopub.execute_input":"2024-04-11T04:21:01.567422Z","iopub.status.idle":"2024-04-11T04:21:01.599680Z","shell.execute_reply.started":"2024-04-11T04:21:01.567398Z","shell.execute_reply":"2024-04-11T04:21:01.598420Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\n# Shuffle your data and labels\nX_train, y_train = shuffle(X_train, y_train, random_state=42)\n\n# Now, X_train is your shuffled data and y_train are the corresponding labels","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:23:33.950896Z","iopub.execute_input":"2024-04-11T04:23:33.951304Z","iopub.status.idle":"2024-04-11T04:23:42.048436Z","shell.execute_reply.started":"2024-04-11T04:23:33.951274Z","shell.execute_reply":"2024-04-11T04:23:42.047638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"np.save('y_train.npy', y_train)\ndel y_train\nnp.save('X_train.npy', X_train)\ndel X_train\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:23:46.733162Z","iopub.execute_input":"2024-04-11T04:23:46.733704Z","iopub.status.idle":"2024-04-11T04:24:40.413080Z","shell.execute_reply.started":"2024-04-11T04:23:46.733671Z","shell.execute_reply":"2024-04-11T04:24:40.411747Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_test_1 = np.load('/kaggle/working/X_test_1.npy')\nX_test_2 = np.load('/kaggle/working/X_test_2.npy')\n\n# # Convert your lists to numpy arrays\n# X_test_1 = np.stack(X_test_1)\n# X_test_2 = np.stack(X_test_2)\n\n# Create labels for your data\ny_test_1 = np.zeros(len(X_test_1))  # Class 0 for X_train_1\ny_test_2 = np.ones(len(X_test_2))   # Class 1 for X_train_2\n\n# Stack your data and labels\nX_test = np.vstack((X_test_1, X_test_2))\ny_test = np.hstack((y_test_1, y_test_2))\n\nfrom sklearn.utils import shuffle\n# Shuffle your data and labels\nX_test, y_test = shuffle(X_test, y_test, random_state=42)\nnp.save('X_test.npy', X_test)\ndel X_test\nnp.save('y_test.npy', y_test)\ndel y_test","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:37:46.386039Z","iopub.execute_input":"2024-04-11T14:37:46.386585Z","iopub.status.idle":"2024-04-11T14:37:58.043391Z","shell.execute_reply.started":"2024-04-11T14:37:46.386555Z","shell.execute_reply":"2024-04-11T14:37:58.042333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import DataLoader #, TensorDataset\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\n\nX_train = np.load('/kaggle/input/x-train-1-2/X_train.npy')\ny_train = np.load('/kaggle/input/x-train-1-2/y_train.npy')\n\n\n# Define your custom dataset\nclass MyDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Create dataset\ntrain_dataset = MyDataset(X_train, y_train)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T15:25:45.798081Z","iopub.execute_input":"2024-04-11T15:25:45.798467Z","iopub.status.idle":"2024-04-11T15:25:45.806310Z","shell.execute_reply.started":"2024-04-11T15:25:45.798437Z","shell.execute_reply":"2024-04-11T15:25:45.805179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create DataLoaders for training and testing\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(65, 4, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256*240, 60)  \n        self.fc3 = nn.Linear(60, 2)\n        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n\n    def forward(self, x):\n        x = x.view(-1, 65, 256, 240)\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = x.view(x.size(0), -1)  # Flatten layer\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n# Initialize the network and print its architecture\nmodel = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:33:04.789840Z","iopub.execute_input":"2024-04-11T17:33:04.790239Z","iopub.status.idle":"2024-04-11T17:33:04.832199Z","shell.execute_reply.started":"2024-04-11T17:33:04.790208Z","shell.execute_reply":"2024-04-11T17:33:04.831299Z"},"trusted":true},"execution_count":236,"outputs":[{"name":"stdout","text":"Net(\n  (conv1): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=61440, out_features=60, bias=True)\n  (fc3): Linear(in_features=60, out_features=2, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Net()\n# Use CUDA if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = \"cpu\"\nmodel.to(device)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0,2.0]).to(device))\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n\n# Initialize empty lists to store losses\ntrain_losses = []\n\n# Training loop\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        l1_lambda = 0.001\n        l1_norm = sum(p.abs().sum() for p in model.parameters())\n        loss = criterion(outputs, labels_onehot)\n        loss+=l1_lambda*l1_norm\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:33:08.724251Z","iopub.execute_input":"2024-04-11T17:33:08.725125Z","iopub.status.idle":"2024-04-11T17:35:55.447555Z","shell.execute_reply.started":"2024-04-11T17:33:08.725092Z","shell.execute_reply":"2024-04-11T17:35:55.446601Z"},"trusted":true},"execution_count":237,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 3.5320\nEpoch 2, training loss: 1.1930\nEpoch 3, training loss: 1.0866\nEpoch 4, training loss: 1.0360\nEpoch 5, training loss: 1.0347\nEpoch 6, training loss: 1.0290\nEpoch 7, training loss: 1.0687\nEpoch 8, training loss: 1.0334\nEpoch 9, training loss: 1.0184\nEpoch 10, training loss: 1.0544\nEpoch 11, training loss: 1.0440\nEpoch 12, training loss: 1.0332\nEpoch 13, training loss: 1.0375\nEpoch 14, training loss: 1.0309\nEpoch 15, training loss: 1.0460\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop without regularization\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels_onehot)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:35:55.449580Z","iopub.execute_input":"2024-04-11T17:35:55.450041Z","iopub.status.idle":"2024-04-11T17:38:42.371999Z","shell.execute_reply.started":"2024-04-11T17:35:55.450005Z","shell.execute_reply":"2024-04-11T17:38:42.370744Z"},"trusted":true},"execution_count":238,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 0.8937\nEpoch 2, training loss: 0.8591\nEpoch 3, training loss: 0.8144\nEpoch 4, training loss: 0.7782\nEpoch 5, training loss: 0.7312\nEpoch 6, training loss: 0.6815\nEpoch 7, training loss: 0.6078\nEpoch 8, training loss: 0.5574\nEpoch 9, training loss: 0.5251\nEpoch 10, training loss: 0.4513\nEpoch 11, training loss: 0.3829\nEpoch 12, training loss: 0.3827\nEpoch 13, training loss: 0.3351\nEpoch 14, training loss: 0.3174\nEpoch 15, training loss: 0.2225\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"# # del X_train, y_train\n# X_test = np.load('/kaggle/input/x-train-1-2/X_test.npy')\n# y_test = np.load('/kaggle/input/x-train-1-2/y_test.npy')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:28:42.191662Z","iopub.execute_input":"2024-04-11T15:28:42.192130Z","iopub.status.idle":"2024-04-11T15:29:06.131351Z","shell.execute_reply.started":"2024-04-11T15:28:42.192102Z","shell.execute_reply":"2024-04-11T15:29:06.130510Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ntest_dataset = MyDataset(X_test, y_test)\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport numpy as np\n\n# Assuming x_test is a PyTorch tensor\ntestloader = torch.utils.data.DataLoader(X_test, batch_size=8)\n\nmodel.eval()  # Set the model to evaluation mode\npredictions = []\noutput = []\nwith torch.no_grad():\n    for data in testloader:\n        outputs = model(data.float().to(device)).to(device)\n        predicted = torch.softmax(outputs, dim = 1) # Apply a threshold\n        predictions.extend(predicted.tolist())\n        output.extend(outputs.tolist())\n\n\ndef find_optimal_threshold(predictions, y_test):\n    min_sum = float('inf')\n    optimal_threshold = 0\n\n    # Iterate over possible thresholds from 0 to 1\n    for threshold in np.arange(0.0, 1.01, 0.001):\n        # Apply threshold\n        preds = (np.array(predictions)[:,1] > threshold).astype(int)\n\n        # Compute confusion matrix\n        cm = confusion_matrix(y_test, preds)\n\n        # Compute sum of off-diagonal elements\n        off_diagonal_sum = 164 - np.trace(cm)\n        #print(cm)\n        # Update optimal threshold if this threshold is better\n        if off_diagonal_sum < min_sum and cm[1][1]/np.sum(cm[1]) >0.5:\n            min_sum = off_diagonal_sum\n            optimal_threshold = threshold\n\n    return optimal_threshold\n\n# Assuming 'predictions' are your predicted probabilities and 'y_test' are your true labels\noptimal_threshold = find_optimal_threshold(predictions, y_test)        \n# Calculate the multi-label confusion matrix\nmcm = confusion_matrix(y_test, (np.array(predictions)[:,1]>optimal_threshold).astype(int))\n\nprint('Confusion Matrix:')\nfor i, matrix in enumerate(mcm):\n    print(f'Class {i}:')\n    print(matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:38:42.373446Z","iopub.execute_input":"2024-04-11T17:38:42.373765Z","iopub.status.idle":"2024-04-11T17:38:46.462656Z","shell.execute_reply.started":"2024-04-11T17:38:42.373736Z","shell.execute_reply":"2024-04-11T17:38:46.461737Z"},"trusted":true},"execution_count":239,"outputs":[{"name":"stdout","text":"Confusion Matrix:\nClass 0:\n[44 38]\nClass 1:\n[18 19]\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T17:38:46.464686Z","iopub.execute_input":"2024-04-11T17:38:46.465028Z","iopub.status.idle":"2024-04-11T17:38:46.475537Z","shell.execute_reply.started":"2024-04-11T17:38:46.464982Z","shell.execute_reply":"2024-04-11T17:38:46.474627Z"},"trusted":true},"execution_count":240,"outputs":[{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"[[0.7328335046768188, 0.26716652512550354],\n [0.02444249764084816, 0.9755574464797974],\n [0.8224502801895142, 0.17754971981048584],\n [0.9875355362892151, 0.01246438268572092],\n [0.9425466060638428, 0.05745336413383484],\n [0.9286924004554749, 0.07130758464336395],\n [0.839780330657959, 0.16021963953971863],\n [0.9614708423614502, 0.0385291762650013],\n [0.9723830819129944, 0.027616865932941437],\n [0.8485738039016724, 0.15142613649368286],\n [0.9778349995613098, 0.02216499298810959],\n [0.9790586829185486, 0.020941313356161118],\n [0.9510384202003479, 0.048961520195007324],\n [0.6394004821777344, 0.3605995774269104],\n [0.879150390625, 0.120849609375],\n [0.9772056341171265, 0.022794289514422417],\n [0.8269427418708801, 0.17305722832679749],\n [0.009851256385445595, 0.990148663520813],\n [0.9271709322929382, 0.07282905280590057],\n [0.9448071122169495, 0.055192895233631134],\n [0.529174268245697, 0.47082576155662537],\n [0.8428623080253601, 0.15713773667812347],\n [0.9924888610839844, 0.007511192932724953],\n [0.947347104549408, 0.05265292152762413],\n [0.8026358485221863, 0.1973641961812973],\n [0.8257394433021545, 0.1742604821920395],\n [0.9781423211097717, 0.021857721731066704],\n [0.8326115608215332, 0.16738848388195038],\n [0.7510921359062195, 0.24890786409378052],\n [0.7374860048294067, 0.26251399517059326],\n [0.7268556356430054, 0.273144394159317],\n [0.9934765696525574, 0.006523413583636284],\n [0.979237973690033, 0.020762043073773384],\n [0.07087919861078262, 0.9291207790374756],\n [0.14032025635242462, 0.8596797585487366],\n [0.4118776023387909, 0.5881223678588867],\n [0.5489259958267212, 0.4510739743709564],\n [0.992782711982727, 0.007217280101031065],\n [0.9122724533081055, 0.08772757649421692],\n [0.9978929162025452, 0.0021071576047688723],\n [0.38021570444107056, 0.6197842955589294],\n [0.9675881266593933, 0.03241187334060669],\n [0.9838093519210815, 0.01619063876569271],\n [0.9442085027694702, 0.055791519582271576],\n [0.2690225839614868, 0.730977475643158],\n [0.9877963066101074, 0.012203676626086235],\n [0.8777675032615662, 0.12223249673843384],\n [0.5948428511619568, 0.4051571786403656],\n [0.4446670413017273, 0.5553329586982727],\n [0.05897238478064537, 0.9410275816917419],\n [0.6019651293754578, 0.39803487062454224],\n [0.7100570797920227, 0.2899429202079773],\n [0.20254358649253845, 0.7974563837051392],\n [0.9360631704330444, 0.06393676996231079],\n [0.733021080493927, 0.2669789791107178],\n [0.7517300844192505, 0.2482699304819107],\n [0.19416260719299316, 0.8058373928070068],\n [0.9632963538169861, 0.03670363873243332],\n [0.9400583505630493, 0.059941619634628296],\n [0.5221575498580933, 0.47784245014190674],\n [0.9084659218788147, 0.0915340706706047],\n [0.9453459978103638, 0.054653964936733246],\n [0.7189366817474365, 0.2810632884502411],\n [0.14341363310813904, 0.8565863966941833],\n [0.9252613186836243, 0.07473861426115036],\n [0.9983766078948975, 0.0016233844216912985],\n [0.683239221572876, 0.3167608082294464],\n [0.10064657032489777, 0.8993534445762634],\n [0.9781978726387024, 0.021802140399813652],\n [0.9886807203292847, 0.011319287121295929],\n [0.9688405394554138, 0.031159430742263794],\n [0.12777847051620483, 0.8722215294837952],\n [0.9724685549736023, 0.027531445026397705],\n [0.045866891741752625, 0.954133152961731],\n [0.9932001233100891, 0.006799850147217512],\n [0.14946851134300232, 0.8505314588546753],\n [0.9083548784255981, 0.09164509177207947],\n [0.23475439846515656, 0.7652455568313599],\n [0.9846466183662415, 0.015353450551629066],\n [0.988387405872345, 0.011612622067332268],\n [0.926792562007904, 0.07320748269557953],\n [0.272210031747818, 0.7277899384498596],\n [0.46175500750541687, 0.5382449626922607],\n [0.6093232035636902, 0.3906767964363098],\n [0.9447296857833862, 0.05527035519480705],\n [0.9573702216148376, 0.042629752308130264],\n [0.9761217832565308, 0.02387825772166252],\n [0.9089375138282776, 0.09106247872114182],\n [0.3649240732192993, 0.6350758671760559],\n [0.9853330254554749, 0.014666969887912273],\n [0.9922075867652893, 0.00779246911406517],\n [0.9829524755477905, 0.01704755611717701],\n [0.11556382477283478, 0.8844361305236816],\n [0.2629132866859436, 0.7370866537094116],\n [0.8349276185035706, 0.16507242619991302],\n [0.9016751050949097, 0.09832490235567093],\n [0.8906897902488708, 0.10931027680635452],\n [0.8870543837547302, 0.11294566094875336],\n [0.8000470995903015, 0.1999528855085373],\n [0.6476253271102905, 0.3523746430873871],\n [0.5269966125488281, 0.4730033874511719],\n [0.492994099855423, 0.5070059299468994],\n [0.9610264897346497, 0.03897356986999512],\n [0.9448302388191223, 0.05516980215907097],\n [0.44569385051727295, 0.554306149482727],\n [0.46295225620269775, 0.5370477437973022],\n [0.5533159971237183, 0.44668400287628174],\n [0.6751299500465393, 0.3248700201511383],\n [0.9857902526855469, 0.01420968770980835],\n [0.15061046183109283, 0.8493895530700684],\n [0.8586181402206421, 0.1413818746805191],\n [0.9132112860679626, 0.08678875863552094],\n [0.8079800605773926, 0.1920199692249298],\n [0.9881935715675354, 0.011806459166109562],\n [0.8392308950424194, 0.16076911985874176],\n [0.8809410929679871, 0.11905888468027115],\n [0.9907281398773193, 0.009271823801100254],\n [0.4946855306625366, 0.5053144693374634],\n [0.9825985431671143, 0.017401425167918205]]"},"metadata":{}}]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:21:45.305105Z","iopub.execute_input":"2024-04-11T17:21:45.305927Z","iopub.status.idle":"2024-04-11T17:21:45.313180Z","shell.execute_reply.started":"2024-04-11T17:21:45.305892Z","shell.execute_reply":"2024-04-11T17:21:45.312165Z"},"trusted":true},"execution_count":220,"outputs":[{"execution_count":220,"output_type":"execute_result","data":{"text/plain":"array([0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n       0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n       0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simpler model\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(65, 2, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128*240, 30)  \n        self.fc3 = nn.Linear(30, 2)\n        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n\n    def forward(self, x):\n        x = x.view(-1, 65, 256, 240)\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = x.view(x.size(0), -1)  # Flatten layer\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n# Initialize the network and print its architecture\nmodel = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:39:39.411023Z","iopub.execute_input":"2024-04-11T17:39:39.411929Z","iopub.status.idle":"2024-04-11T17:39:39.429575Z","shell.execute_reply.started":"2024-04-11T17:39:39.411893Z","shell.execute_reply":"2024-04-11T17:39:39.428740Z"},"trusted":true},"execution_count":241,"outputs":[{"name":"stdout","text":"Net(\n  (conv1): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=30720, out_features=30, bias=True)\n  (fc3): Linear(in_features=30, out_features=2, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use CUDA if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = \"cpu\"\nmodel.to(device)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0,3.0]).to(device))\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n\n# Initialize empty lists to store losses\ntrain_losses = []\n\n# Training loop\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        l1_lambda = 0.001\n        l1_norm = sum(p.abs().sum() for p in model.parameters())\n        loss = criterion(outputs, labels_onehot)\n        loss+=l1_lambda*l1_norm\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:11:48.733055Z","iopub.execute_input":"2024-04-11T18:11:48.733404Z","iopub.status.idle":"2024-04-11T18:14:28.439297Z","shell.execute_reply.started":"2024-04-11T18:11:48.733375Z","shell.execute_reply":"2024-04-11T18:14:28.438291Z"},"trusted":true},"execution_count":247,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 1.9654\nEpoch 2, training loss: 1.4470\nEpoch 3, training loss: 1.2368\nEpoch 4, training loss: 1.1353\nEpoch 5, training loss: 1.0957\nEpoch 6, training loss: 1.0755\nEpoch 7, training loss: 1.0677\nEpoch 8, training loss: 0.9710\nEpoch 9, training loss: 1.0508\nEpoch 10, training loss: 0.9714\nEpoch 11, training loss: 1.0511\nEpoch 12, training loss: 0.9834\nEpoch 13, training loss: 0.9784\nEpoch 14, training loss: 0.9487\nEpoch 15, training loss: 0.9369\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"### optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        #l1_lambda = 0.001\n        #l1_norm = sum(p.abs().sum() for p in model.parameters())\n        loss = criterion(outputs, labels_onehot)\n        #loss+=l1_lambda*l1_norm\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:14:55.562986Z","iopub.execute_input":"2024-04-11T18:14:55.563345Z","iopub.status.idle":"2024-04-11T18:17:35.006044Z","shell.execute_reply.started":"2024-04-11T18:14:55.563315Z","shell.execute_reply":"2024-04-11T18:17:35.004935Z"},"trusted":true},"execution_count":248,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 0.3254\nEpoch 2, training loss: 0.2787\nEpoch 3, training loss: 0.2563\nEpoch 4, training loss: 0.1999\nEpoch 5, training loss: 0.2448\nEpoch 6, training loss: 0.2003\nEpoch 7, training loss: 0.1866\nEpoch 8, training loss: 0.1581\nEpoch 9, training loss: 0.1975\nEpoch 10, training loss: 0.1601\nEpoch 11, training loss: 0.1733\nEpoch 12, training loss: 0.1513\nEpoch 13, training loss: 0.1048\nEpoch 14, training loss: 0.1637\nEpoch 15, training loss: 0.1306\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntest_dataset = MyDataset(X_test, y_test)\nfrom sklearn.metrics import confusion_matrix\n\n# Assuming x_test is a PyTorch tensor\ntestloader = torch.utils.data.DataLoader(X_test, batch_size=8)\n\nmodel.eval()  # Set the model to evaluation mode\npredictions = []\noutput = []\nwith torch.no_grad():\n    for data in testloader:\n        outputs = model(data.float().to(device)).to(device)\n        predicted = torch.softmax(outputs, dim = 1) # Apply a threshold\n        predictions.extend(predicted.tolist())\n        output.extend(outputs.tolist())\n\n        \noptimal_threshold = find_optimal_threshold(predictions, y_test)\n# Calculate the multi-label confusion matrix\nmcm = confusion_matrix(y_test, (np.array(predictions)[:,1]>optimal_threshold).astype(int))\n\nprint('Confusion Matrix:')\nfor i, matrix in enumerate(mcm):\n    print(f'Class {i}:')\n    print(matrix)\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T18:21:22.320658Z","iopub.execute_input":"2024-04-11T18:21:22.321067Z","iopub.status.idle":"2024-04-11T18:21:26.776827Z","shell.execute_reply.started":"2024-04-11T18:21:22.321036Z","shell.execute_reply":"2024-04-11T18:21:26.775841Z"},"trusted":true},"execution_count":249,"outputs":[{"name":"stdout","text":"Confusion Matrix:\nClass 0:\n[53 29]\nClass 1:\n[17 20]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}