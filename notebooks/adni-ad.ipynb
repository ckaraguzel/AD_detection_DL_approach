{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(0)\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "# Create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=30720, out_features=65, bias=True)\n",
      "  (fc3): Linear(in_features=65, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simpler model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(65, 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128*240, 65)  \n",
    "        self.fc3 = nn.Linear(65, 2)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 65, 256, 240)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = x.view(x.size(0), -1)  # Flatten layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# Initialize the network and print its architecture\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 3.0144\n",
      "Epoch 2, training loss: 2.0268\n",
      "Epoch 3, training loss: 1.3780\n",
      "Epoch 4, training loss: 1.0252\n",
      "Epoch 5, training loss: 0.8956\n",
      "Epoch 6, training loss: 0.8356\n",
      "Epoch 7, training loss: 0.8230\n",
      "Epoch 8, training loss: 0.8027\n",
      "Epoch 9, training loss: 0.8040\n",
      "Epoch 10, training loss: 0.8018\n",
      "Epoch 11, training loss: 0.7677\n",
      "Epoch 12, training loss: 0.7583\n",
      "Epoch 13, training loss: 0.7618\n",
      "Epoch 14, training loss: 0.7770\n",
      "Epoch 15, training loss: 0.7990\n",
      "Epoch 1, training loss: 0.7308\n",
      "Epoch 2, training loss: 0.6901\n",
      "Epoch 3, training loss: 0.6770\n",
      "Epoch 4, training loss: 0.6147\n",
      "Epoch 5, training loss: 0.5677\n",
      "Epoch 6, training loss: 0.4948\n",
      "Epoch 7, training loss: 0.4114\n",
      "Epoch 8, training loss: 0.3996\n",
      "Epoch 9, training loss: 0.3288\n",
      "Epoch 10, training loss: 0.2399\n",
      "Epoch 11, training loss: 0.2265\n",
      "Epoch 12, training loss: 0.1683\n",
      "Epoch 13, training loss: 0.1567\n",
      "Epoch 14, training loss: 0.1151\n",
      "Epoch 15, training loss: 0.1041\n",
      "Epoch 16, training loss: 0.1024\n",
      "Epoch 17, training loss: 0.0746\n",
      "Epoch 18, training loss: 0.0625\n",
      "Epoch 19, training loss: 0.0756\n",
      "Epoch 20, training loss: 0.0553\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([0.5,1.0]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Initialize empty lists to store losses\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        l1_lambda = 0.0005\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss+=l1_lambda*l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Compute training loss\n",
    "        train_losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "# Now without regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Compute training loss\n",
    "        train_losses.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Class 0:\n",
      "[57 25]\n",
      "Class 1:\n",
      "[16 21]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming x_test is a PyTorch tensor\n",
    "testloader = torch.utils.data.DataLoader(X_test, batch_size=32)\n",
    "def find_optimal_threshold(predictions, y_test):\n",
    "    min_sum = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    # Iterate over possible thresholds from 0 to 1\n",
    "    for threshold in np.arange(0.0, 1, 0.001):\n",
    "        # Apply threshold\n",
    "        preds = (np.array(predictions)[:,1] > threshold).astype(int)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "        # Compute sum of off-diagonal elements\n",
    "        off_diagonal_sum = 164 - np.trace(cm)\n",
    "        #print(cm)\n",
    "        # Update optimal threshold if this threshold is better\n",
    "        if off_diagonal_sum < min_sum and cm[1][1]/np.sum(cm[1]) >0.5:\n",
    "            min_sum = off_diagonal_sum\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "output = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        outputs = model(data.float().to(device)).to(device)\n",
    "        predicted = torch.softmax(outputs, dim = 1) # Apply a threshold\n",
    "        predictions.extend(predicted.tolist())\n",
    "        output.extend(outputs.tolist())\n",
    "\n",
    "        \n",
    "optimal_threshold = find_optimal_threshold(predictions, y_test)\n",
    "# Calculate the multi-label confusion matrix\n",
    "mcm = confusion_matrix(y_test, (np.array(predictions)[:,1]>optimal_threshold).astype(int))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "for i, matrix in enumerate(mcm):\n",
    "    print(f'Class {i}:')\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6313447593935398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn\n",
    "outputs_np = (np.array(predictions)[:,1]>optimal_threshold).astype(int)\n",
    "labels_np = y_test\n",
    "\n",
    "auc_score = roc_auc_score(labels_np, outputs_np)\n",
    "print(f\"AUC Score: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8209069,
     "datasetId": 4773921,
     "sourceId": 8092331,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
