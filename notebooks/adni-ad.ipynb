{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8092331,"datasetId":4773921,"databundleVersionId":8209069}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T14:27:09.387207Z","iopub.execute_input":"2024-04-11T14:27:09.387772Z","iopub.status.idle":"2024-04-11T14:27:09.392869Z","shell.execute_reply.started":"2024-04-11T14:27:09.387739Z","shell.execute_reply":"2024-04-11T14:27:09.392001Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/adni-sorted/pixels_CN_1.pkl', 'rb') as f:\n    cn_1 = pickle.load(f)\nwith open('/kaggle/input/adni-sorted/pixels_CN_2.pkl', 'rb') as f:\n    cn_2 = pickle.load(f)\nwith open('/kaggle/input/adni-sorted/pixels_MCI.pkl', 'rb') as f:\n    mci = pickle.load(f)\n    \ncn_1.extend(cn_2)\n\nmci.pop(70)\ndel cn_2\nprint('dataset loaded')\n## Change the shape of patient 2, slice 6 in MCI","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:27:11.695122Z","iopub.execute_input":"2024-04-11T14:27:11.695431Z","iopub.status.idle":"2024-04-11T14:28:00.313167Z","shell.execute_reply.started":"2024-04-11T14:27:11.695408Z","shell.execute_reply":"2024-04-11T14:28:00.312219Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"dataset loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:12:17.858540Z","iopub.execute_input":"2024-04-10T18:12:17.859384Z","iopub.status.idle":"2024-04-10T18:12:17.863296Z","shell.execute_reply.started":"2024-04-10T18:12:17.859342Z","shell.execute_reply":"2024-04-10T18:12:17.862249Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\n\n# Set the seed\nrandom.seed(42)\n# Calculate the number of samples\nl_cn = len(cn_1)\nl_mci = len(mci)\nnum_samples_1 = int(0.8 * l_cn)\nnum_samples_2 = int(0.8* l_mci)\n\ntrain_indices_1 = random.sample(list(range(l_cn)), num_samples_1)\ntrain_indices_2 = random.sample(list(range(l_mci)), num_samples_2)\n\ntest_indices_1 = list(set(list(range(l_cn))) - set(train_indices_1))\ntest_indices_2 = list(set(list(range(l_mci))) - set(train_indices_2))\n\ndef scale(arr):\n    return (arr - np.min(arr))/(np.max(arr) - np.min(arr))\n\n# # Sample 80% of the entries\n# X_train_1 = [np.array([scale(cn_1[i][j]) for j in range(65)]) for i in train_indices_1]\n# np.save('X_train_1.npy',np.array(X_train_1))\n# X_train_2 = [np.array([scale(mci[i][j]) for j in range(65)]) for i in train_indices_2]\n# np.save('X_train_2.npy', np.array(X_train_2))\n\n# del X_train_1, X_train_2, train_indices_1, train_indices_2\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T14:28:00.315054Z","iopub.execute_input":"2024-04-11T14:28:00.315483Z","iopub.status.idle":"2024-04-11T14:28:00.323878Z","shell.execute_reply.started":"2024-04-11T14:28:00.315448Z","shell.execute_reply":"2024-04-11T14:28:00.322886Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_test_1 = [np.array([scale(cn_1[i][j]) for j in range(65)]) for i in test_indices_1]\nnp.save('X_test_1.npy',np.array(X_test_1))\nX_test_2 = [np.array([scale(mci[i][j]) for j in range(65)]) for i in test_indices_2]\nnp.save('X_test_2.npy',np.array(X_test_2))\n\ndel X_test_1, X_test_2, test_indices_1, test_indices_2\n#del mci, cn_1","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:28:00.324927Z","iopub.execute_input":"2024-04-11T14:28:00.325176Z","iopub.status.idle":"2024-04-11T14:28:08.768003Z","shell.execute_reply.started":"2024-04-11T14:28:00.325154Z","shell.execute_reply":"2024-04-11T14:28:08.767100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_1 = np.load('/kaggle/input/x-train-1-2/X_train_1.npy')\nX_train_2 = np.load('/kaggle/input/x-train-1-2/X_train_2.npy')\n\n# Create labels for your data\ny_train_1 = np.zeros(len(X_train_1))  # Class 0 for X_train_1\ny_train_2 = np.ones(len(X_train_2))   # Class 1 for X_train_2\n\n# Stack your data and labels\nX_train = np.vstack((X_train_1, X_train_2))\ny_train = np.hstack((y_train_1, y_train_2))","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:18:39.752475Z","iopub.execute_input":"2024-04-11T04:18:39.753000Z","iopub.status.idle":"2024-04-11T04:21:01.565502Z","shell.execute_reply.started":"2024-04-11T04:18:39.752960Z","shell.execute_reply":"2024-04-11T04:21:01.564430Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"del X_train_1, X_train_2, y_train_1, y_train_2","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:21:01.567131Z","iopub.execute_input":"2024-04-11T04:21:01.567422Z","iopub.status.idle":"2024-04-11T04:21:01.599680Z","shell.execute_reply.started":"2024-04-11T04:21:01.567398Z","shell.execute_reply":"2024-04-11T04:21:01.598420Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\n# Shuffle your data and labels\nX_train, y_train = shuffle(X_train, y_train, random_state=42)\n\n# Now, X_train is your shuffled data and y_train are the corresponding labels","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:23:33.950896Z","iopub.execute_input":"2024-04-11T04:23:33.951304Z","iopub.status.idle":"2024-04-11T04:23:42.048436Z","shell.execute_reply.started":"2024-04-11T04:23:33.951274Z","shell.execute_reply":"2024-04-11T04:23:42.047638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"np.save('y_train.npy', y_train)\ndel y_train\nnp.save('X_train.npy', X_train)\ndel X_train\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:23:46.733162Z","iopub.execute_input":"2024-04-11T04:23:46.733704Z","iopub.status.idle":"2024-04-11T04:24:40.413080Z","shell.execute_reply.started":"2024-04-11T04:23:46.733671Z","shell.execute_reply":"2024-04-11T04:24:40.411747Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_test_1 = np.load('/kaggle/working/X_test_1.npy')\nX_test_2 = np.load('/kaggle/working/X_test_2.npy')\n\n# # Convert your lists to numpy arrays\n# X_test_1 = np.stack(X_test_1)\n# X_test_2 = np.stack(X_test_2)\n\n# Create labels for your data\ny_test_1 = np.zeros(len(X_test_1))  # Class 0 for X_train_1\ny_test_2 = np.ones(len(X_test_2))   # Class 1 for X_train_2\n\n# Stack your data and labels\nX_test = np.vstack((X_test_1, X_test_2))\ny_test = np.hstack((y_test_1, y_test_2))\n\nfrom sklearn.utils import shuffle\n# Shuffle your data and labels\nX_test, y_test = shuffle(X_test, y_test, random_state=42)\nnp.save('X_test.npy', X_test)\ndel X_test\nnp.save('y_test.npy', y_test)\ndel y_test","metadata":{"execution":{"iopub.status.busy":"2024-04-11T14:37:46.386039Z","iopub.execute_input":"2024-04-11T14:37:46.386585Z","iopub.status.idle":"2024-04-11T14:37:58.043391Z","shell.execute_reply.started":"2024-04-11T14:37:46.386555Z","shell.execute_reply":"2024-04-11T14:37:58.042333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import DataLoader #, TensorDataset\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\n\nX_train = np.load('/kaggle/input/x-train-1-2/X_train.npy')\ny_train = np.load('/kaggle/input/x-train-1-2/y_train.npy')\n\n\n# Define your custom dataset\nclass MyDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Create dataset\ntrain_dataset = MyDataset(X_train, y_train)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T15:25:45.798081Z","iopub.execute_input":"2024-04-11T15:25:45.798467Z","iopub.status.idle":"2024-04-11T15:25:45.806310Z","shell.execute_reply.started":"2024-04-11T15:25:45.798437Z","shell.execute_reply":"2024-04-11T15:25:45.805179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create DataLoaders for training and testing\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(65, 4, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(256*240, 60)  \n        self.fc3 = nn.Linear(60, 2)\n        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n\n    def forward(self, x):\n        x = x.view(-1, 65, 256, 240)\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = x.view(x.size(0), -1)  # Flatten layer\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n# Initialize the network and print its architecture\nmodel = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:47:31.069518Z","iopub.execute_input":"2024-04-11T16:47:31.069880Z","iopub.status.idle":"2024-04-11T16:47:31.118125Z","shell.execute_reply.started":"2024-04-11T16:47:31.069838Z","shell.execute_reply":"2024-04-11T16:47:31.117142Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Net(\n  (conv1): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=61440, out_features=60, bias=True)\n  (fc3): Linear(in_features=60, out_features=2, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Net()\n# Use CUDA if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = \"cpu\"\nmodel.to(device)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0,3.0]).to(device))\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n\n# Initialize empty lists to store losses\ntrain_losses = []\n\n# Training loop\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        l1_lambda = 0.001\n        l1_norm = sum(p.abs().sum() for p in model.parameters())\n        loss = criterion(outputs, labels_onehot)\n        loss+=l1_lambda*l1_norm\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:51:25.305986Z","iopub.execute_input":"2024-04-11T16:51:25.306604Z","iopub.status.idle":"2024-04-11T16:54:15.209796Z","shell.execute_reply.started":"2024-04-11T16:51:25.306573Z","shell.execute_reply":"2024-04-11T16:54:15.208912Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 4.0177\nEpoch 2, training loss: 1.6383\nEpoch 3, training loss: 1.4246\nEpoch 4, training loss: 1.3309\nEpoch 5, training loss: 1.3224\nEpoch 6, training loss: 1.3238\nEpoch 7, training loss: 1.3003\nEpoch 8, training loss: 1.2996\nEpoch 9, training loss: 1.3065\nEpoch 10, training loss: 1.3069\nEpoch 11, training loss: 1.3004\nEpoch 12, training loss: 1.3308\nEpoch 13, training loss: 1.3286\nEpoch 14, training loss: 1.2968\nEpoch 15, training loss: 1.2781\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nfor epoch in range(10):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels_onehot)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:42:39.854534Z","iopub.execute_input":"2024-04-11T16:42:39.855259Z","iopub.status.idle":"2024-04-11T16:44:36.030591Z","shell.execute_reply.started":"2024-04-11T16:42:39.855225Z","shell.execute_reply":"2024-04-11T16:44:36.029566Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 0.2936\nEpoch 2, training loss: 0.3397\nEpoch 3, training loss: 0.2719\nEpoch 4, training loss: 0.2183\nEpoch 5, training loss: 0.2191\nEpoch 6, training loss: 0.2743\nEpoch 7, training loss: 0.2163\nEpoch 8, training loss: 0.2140\nEpoch 9, training loss: 0.2708\nEpoch 10, training loss: 0.1823\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"# del X_train, y_train\nX_test = np.load('/kaggle/input/x-train-1-2/X_test.npy')\ny_test = np.load('/kaggle/input/x-train-1-2/y_test.npy')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T15:28:42.191662Z","iopub.execute_input":"2024-04-11T15:28:42.192130Z","iopub.status.idle":"2024-04-11T15:29:06.131351Z","shell.execute_reply.started":"2024-04-11T15:28:42.192102Z","shell.execute_reply":"2024-04-11T15:29:06.130510Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ntest_dataset = MyDataset(X_test, y_test)\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport numpy as np\n\n# Assuming x_test is a PyTorch tensor\ntestloader = torch.utils.data.DataLoader(X_test, batch_size=8)\n\nmodel.eval()  # Set the model to evaluation mode\npredictions = []\noutput = []\nwith torch.no_grad():\n    for data in testloader:\n        outputs = model(data.float().to(device)).to(device)\n        predicted = torch.softmax(outputs, dim = 1) # Apply a threshold\n        predictions.extend(predicted.tolist())\n        output.extend(outputs.tolist())\n\n# Calculate the multi-label confusion matrix\nmcm = confusion_matrix(y_test, (np.array(predictions)[:,1]>0.5).astype(int))\n\nprint('Confusion Matrix:')\nfor i, matrix in enumerate(mcm):\n    print(f'Class {i}:')\n    print(matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:58:07.136755Z","iopub.execute_input":"2024-04-11T16:58:07.137119Z","iopub.status.idle":"2024-04-11T16:58:10.164070Z","shell.execute_reply.started":"2024-04-11T16:58:07.137088Z","shell.execute_reply":"2024-04-11T16:58:10.163048Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stdout","text":"Confusion Matrix:\nClass 0:\n[33 49]\nClass 1:\n[17 20]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-11T16:55:00.091574Z","iopub.execute_input":"2024-04-11T16:55:00.092010Z","iopub.status.idle":"2024-04-11T16:55:00.103091Z","shell.execute_reply.started":"2024-04-11T16:55:00.091976Z","shell.execute_reply":"2024-04-11T16:55:00.102214Z"},"trusted":true},"execution_count":159,"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"[[0.48370444774627686, 0.5162955522537231],\n [0.3163543939590454, 0.6836456060409546],\n [0.46945518255233765, 0.5305447578430176],\n [0.4875570833683014, 0.5124428868293762],\n [0.47460654377937317, 0.5253934860229492],\n [0.5116870403289795, 0.48831290006637573],\n [0.5296944975852966, 0.4703054428100586],\n [0.5188472867012024, 0.48115274310112],\n [0.5343288779258728, 0.4656711220741272],\n [0.32310184836387634, 0.676898181438446],\n [0.47396770119667053, 0.5260322690010071],\n [0.5028485655784607, 0.4971514940261841],\n [0.5245201587677002, 0.4754798114299774],\n [0.5126549601554871, 0.48734503984451294],\n [0.5073081851005554, 0.4926917850971222],\n [0.49869900941848755, 0.5013009905815125],\n [0.48506057262420654, 0.5149394869804382],\n [0.3344052731990814, 0.6655946969985962],\n [0.4891047775745392, 0.5108952522277832],\n [0.48636606335639954, 0.5136339664459229],\n [0.4756859540939331, 0.5243140459060669],\n [0.529448390007019, 0.47055158019065857],\n [0.49672931432724, 0.50327068567276],\n [0.5250546336174011, 0.47494539618492126],\n [0.49853724241256714, 0.5014628171920776],\n [0.49792999029159546, 0.5020700097084045],\n [0.52308589220047, 0.4769141376018524],\n [0.47500959038734436, 0.5249903798103333],\n [0.42579180002212524, 0.5742081999778748],\n [0.4558008015155792, 0.5441992282867432],\n [0.3974088728427887, 0.6025910973548889],\n [0.5142297148704529, 0.48577022552490234],\n [0.5186243057250977, 0.48137566447257996],\n [0.3041478991508484, 0.6958520412445068],\n [0.4148108959197998, 0.585189163684845],\n [0.3837089538574219, 0.6162910461425781],\n [0.4732559025287628, 0.5267440676689148],\n [0.5008688569068909, 0.49913108348846436],\n [0.5289081335067749, 0.4710918962955475],\n [0.5383353233337402, 0.46166467666625977],\n [0.49582618474960327, 0.5041738152503967],\n [0.5105742812156677, 0.4894257187843323],\n [0.5031548738479614, 0.49684515595436096],\n [0.4557393491268158, 0.5442606806755066],\n [0.44880157709121704, 0.551198422908783],\n [0.5301500558853149, 0.46984991431236267],\n [0.47470492124557495, 0.525295078754425],\n [0.4810832738876343, 0.518916666507721],\n [0.5037205815315247, 0.49627944827079773],\n [0.4867173731327057, 0.5132826566696167],\n [0.3993527293205261, 0.6006472110748291],\n [0.45737674832344055, 0.5426232218742371],\n [0.44794946908950806, 0.5520505309104919],\n [0.47431111335754395, 0.525688886642456],\n [0.5064365267753601, 0.4935634434223175],\n [0.5078149437904358, 0.4921850264072418],\n [0.4674237370491028, 0.5325762629508972],\n [0.516610860824585, 0.48338913917541504],\n [0.5371040105819702, 0.4628959894180298],\n [0.4640079140663147, 0.5359921455383301],\n [0.4865892827510834, 0.513410747051239],\n [0.4722413718700409, 0.5277586579322815],\n [0.4374220371246338, 0.5625779628753662],\n [0.42398202419281006, 0.5760179162025452],\n [0.5271148681640625, 0.4728851914405823],\n [0.5137275457382202, 0.4862724840641022],\n [0.4604462683200836, 0.539553701877594],\n [0.36437058448791504, 0.6356293559074402],\n [0.5198010802268982, 0.4801988899707794],\n [0.5075052380561829, 0.4924947917461395],\n [0.4903424382209778, 0.5096575617790222],\n [0.467303067445755, 0.5326969623565674],\n [0.5116211771965027, 0.4883788526058197],\n [0.32407382130622864, 0.6759262084960938],\n [0.500999391078949, 0.499000608921051],\n [0.41302981972694397, 0.5869701504707336],\n [0.5077966451644897, 0.4922032952308655],\n [0.4155491292476654, 0.5844508409500122],\n [0.5101825594902039, 0.48981744050979614],\n [0.47070422768592834, 0.529295802116394],\n [0.4967776834964752, 0.5032222867012024],\n [0.48042410612106323, 0.5195758938789368],\n [0.44654276967048645, 0.5534572601318359],\n [0.5097899436950684, 0.49021002650260925],\n [0.5227985978126526, 0.4772014021873474],\n [0.4884839653968811, 0.5115160346031189],\n [0.5017497539520264, 0.49825021624565125],\n [0.4735536575317383, 0.5264462828636169],\n [0.43013083934783936, 0.5698691606521606],\n [0.530418336391449, 0.4695816934108734],\n [0.5262198448181152, 0.47378015518188477],\n [0.5078802704811096, 0.4921196699142456],\n [0.30211353302001953, 0.6978864669799805],\n [0.38658857345581055, 0.6134114265441895],\n [0.47196635603904724, 0.5280336141586304],\n [0.45301753282546997, 0.5469825267791748],\n [0.5090417861938477, 0.49095821380615234],\n [0.5107740759849548, 0.48922595381736755],\n [0.4749133884906769, 0.5250866413116455],\n [0.4299800395965576, 0.5700199007987976],\n [0.32754436135292053, 0.6724556088447571],\n [0.433383971452713, 0.5666159987449646],\n [0.477757066488266, 0.5222429633140564],\n [0.5086680054664612, 0.4913319945335388],\n [0.481174111366272, 0.518825888633728],\n [0.5003616809844971, 0.49963825941085815],\n [0.5110482573509216, 0.4889516830444336],\n [0.47725415229797363, 0.5227458477020264],\n [0.5256155133247375, 0.47438445687294006],\n [0.4634302854537964, 0.5365696549415588],\n [0.5043538808822632, 0.4956461787223816],\n [0.5118054151535034, 0.4881945848464966],\n [0.5266275405883789, 0.4733724892139435],\n [0.49688342213630676, 0.5031165480613708],\n [0.4900253117084503, 0.5099747180938721],\n [0.533099353313446, 0.46690064668655396],\n [0.5281427502632141, 0.4718572497367859],\n [0.4522826671600342, 0.5477173924446106],\n [0.5033845901489258, 0.49661538004875183]]"},"metadata":{}}]},{"cell_type":"code","source":"def find_optimal_threshold(predictions, y_test):\n    min_sum = float('inf')\n    optimal_threshold = 0\n\n    # Iterate over possible thresholds from 0 to 1\n    for threshold in np.arange(0.0, 1.01, 0.001):\n        # Apply threshold\n        preds = (np.array(predictions)[:,1] > threshold).astype(int)\n\n        # Compute confusion matrix\n        cm = confusion_matrix(y_test, preds)\n\n        # Compute sum of off-diagonal elements\n        off_diagonal_sum = 164 - cm[0][0]*0.52 - (cm[1][1])\n        #print(cm)\n        # Update optimal threshold if this threshold is better\n        if off_diagonal_sum < min_sum:\n            min_sum = off_diagonal_sum\n            optimal_threshold = threshold\n\n    return optimal_threshold\n\n# Assuming 'predictions' are your predicted probabilities and 'y_test' are your true labels\noptimal_threshold = find_optimal_threshold(predictions, y_test)\n\nprint(f\"The optimal threshold is {optimal_threshold}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:57:48.764296Z","iopub.execute_input":"2024-04-11T16:57:48.764958Z","iopub.status.idle":"2024-04-11T16:57:49.885622Z","shell.execute_reply.started":"2024-04-11T16:57:48.764919Z","shell.execute_reply":"2024-04-11T16:57:49.884633Z"},"trusted":true},"execution_count":179,"outputs":[{"name":"stdout","text":"The optimal threshold is 0.487\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:45:23.591451Z","iopub.execute_input":"2024-04-11T16:45:23.591821Z","iopub.status.idle":"2024-04-11T16:45:23.599933Z","shell.execute_reply.started":"2024-04-11T16:45:23.591791Z","shell.execute_reply":"2024-04-11T16:45:23.598923Z"},"trusted":true},"execution_count":148,"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"array([0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n       0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n       0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simpler model\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(65, 2, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128*240, 30)  \n        self.fc3 = nn.Linear(30, 2)\n        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n\n    def forward(self, x):\n        x = x.view(-1, 65, 256, 240)\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = x.view(x.size(0), -1)  # Flatten layer\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n# Initialize the network and print its architecture\nmodel = Net()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:58:51.601189Z","iopub.execute_input":"2024-04-11T16:58:51.601947Z","iopub.status.idle":"2024-04-11T16:58:51.620734Z","shell.execute_reply.started":"2024-04-11T16:58:51.601911Z","shell.execute_reply":"2024-04-11T16:58:51.619901Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stdout","text":"Net(\n  (conv1): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=30720, out_features=30, bias=True)\n  (fc3): Linear(in_features=30, out_features=2, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use CUDA if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = \"cpu\"\nmodel.to(device)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0,3.0]).to(device))\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n\n# Initialize empty lists to store losses\ntrain_losses = []\n\n# Training loop\nfor epoch in range(15):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        labels_onehot = torch.zeros(labels.size(0), 2, device=device)\n        labels = labels.long()\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        l1_lambda = 0.001\n        l1_norm = sum(p.abs().sum() for p in model.parameters())\n        loss = criterion(outputs, labels_onehot)\n        loss+=l1_lambda*l1_norm\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader):.4f}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T16:58:53.311227Z","iopub.execute_input":"2024-04-11T16:58:53.311553Z","iopub.status.idle":"2024-04-11T17:01:34.475622Z","shell.execute_reply.started":"2024-04-11T16:58:53.311528Z","shell.execute_reply":"2024-04-11T17:01:34.474682Z"},"trusted":true},"execution_count":185,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 2.3303\nEpoch 2, training loss: 1.3725\nEpoch 3, training loss: 1.2508\nEpoch 4, training loss: 1.2252\nEpoch 5, training loss: 1.1961\nEpoch 6, training loss: 1.1935\nEpoch 7, training loss: 1.1927\nEpoch 8, training loss: 1.2053\nEpoch 9, training loss: 1.1870\nEpoch 10, training loss: 1.1795\nEpoch 11, training loss: 1.1523\nEpoch 12, training loss: 1.1647\nEpoch 13, training loss: 1.1614\nEpoch 14, training loss: 1.1556\nEpoch 15, training loss: 1.1483\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntest_dataset = MyDataset(X_test, y_test)\nfrom sklearn.metrics import confusion_matrix\n\n# Assuming x_test is a PyTorch tensor\ntestloader = torch.utils.data.DataLoader(X_test, batch_size=8)\n\nmodel.eval()  # Set the model to evaluation mode\npredictions = []\noutput = []\nwith torch.no_grad():\n    for data in testloader:\n        outputs = model(data.float().to(device)).to(device)\n        predicted = torch.softmax(outputs, dim = 1) # Apply a threshold\n        predictions.extend(predicted.tolist())\n        output.extend(outputs.tolist())\n\n# Calculate the multi-label confusion matrix\nmcm = confusion_matrix(y_test, (np.array(predictions)[:,1]>0.538).astype(int))\n\nprint('Confusion Matrix:')\nfor i, matrix in enumerate(mcm):\n    print(f'Class {i}:')\n    print(matrix)\n\n    \ndef find_optimal_threshold(predictions, y_test):\n    min_sum = float('inf')\n    optimal_threshold = 0\n\n    # Iterate over possible thresholds from 0 to 1\n    for threshold in np.arange(0.0, 1.01, 0.001):\n        # Apply threshold\n        preds = (np.array(predictions)[:,1] > threshold).astype(int)\n\n        # Compute confusion matrix\n        cm = confusion_matrix(y_test, preds)\n\n        # Compute sum of off-diagonal elements\n        off_diagonal_sum = 164 - cm[0][0]*0.5 - (cm[1][1])\n        #print(cm)\n        # Update optimal threshold if this threshold is better\n        if off_diagonal_sum < min_sum:\n            min_sum = off_diagonal_sum\n            optimal_threshold = threshold\n\n    return optimal_threshold    \n#Assuming 'predictions' are your predicted probabilities and 'y_test' are your true labels\noptimal_threshold = find_optimal_threshold(predictions, y_test)\n\nprint(f\"The optimal threshold is {optimal_threshold}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-11T17:05:41.098941Z","iopub.execute_input":"2024-04-11T17:05:41.099336Z","iopub.status.idle":"2024-04-11T17:05:45.336414Z","shell.execute_reply.started":"2024-04-11T17:05:41.099304Z","shell.execute_reply":"2024-04-11T17:05:45.335417Z"},"trusted":true},"execution_count":190,"outputs":[{"name":"stdout","text":"Confusion Matrix:\nClass 0:\n[40 42]\nClass 1:\n[13 24]\nThe optimal threshold is 0.538\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}