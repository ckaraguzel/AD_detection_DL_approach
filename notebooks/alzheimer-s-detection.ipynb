{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport os\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"5e3bbedd-c862-4060-9174-8f328e43f721","_cell_guid":"28fc7583-37c8-4c12-a0d5-77a2e08b6c48","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-28T13:38:44.377889Z","iopub.execute_input":"2024-03-28T13:38:44.378291Z","iopub.status.idle":"2024-03-28T13:38:47.841824Z","shell.execute_reply.started":"2024-03-28T13:38:44.378236Z","shell.execute_reply":"2024-03-28T13:38:47.840552Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path1 = []\npath2 = []\npath3 = []\npath0 = []\nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Non Demented'):\n    for filename in filenames:\n        path0.append(os.path.join(dirname, filename))\n        \nprint('First done')\nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Mild Dementia'):\n    for filename in filenames:\n        path1.append(os.path.join(dirname, filename))\n        \nprint('Second done')\nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Moderate Dementia'):\n    for filename in filenames:\n        path3.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Very mild Dementia'):\n    for filename in filenames:\n        path2.append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:38:50.669579Z","iopub.execute_input":"2024-03-28T13:38:50.670243Z","iopub.status.idle":"2024-03-28T13:41:11.457198Z","shell.execute_reply.started":"2024-03-28T13:38:50.670205Z","shell.execute_reply":"2024-03-28T13:41:11.455629Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"First done\nSecond done\n","output_type":"stream"}]},{"cell_type":"code","source":"4+6","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:41:11.459341Z","iopub.execute_input":"2024-03-28T13:41:11.459787Z","iopub.status.idle":"2024-03-28T13:41:11.469155Z","shell.execute_reply.started":"2024-03-28T13:41:11.459752Z","shell.execute_reply":"2024-03-28T13:41:11.467761Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"def create_dict(file_names):\n    img_dict = {}\n    for name in file_names:\n        ind = name[:-8]\n        if ind in img_dict:\n            img_dict[ind].append(name)\n        else:\n            dem = name[31:34]\n            if dem=='Non':\n                img_dict[ind] = [0]\n            elif dem == 'Mil':\n                img_dict[ind] = [1]\n            elif dem == 'Ver':\n                img_dict[ind] = [2]\n            else:\n                img_dict[ind] = [3]\n            img_dict[ind].append(name)\n    return img_dict\nimg_dict = create_dict(path0 + path1 + path2 + path3)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:45:14.189093Z","iopub.execute_input":"2024-03-28T13:45:14.189679Z","iopub.status.idle":"2024-03-28T13:45:14.264746Z","shell.execute_reply.started":"2024-03-28T13:45:14.189640Z","shell.execute_reply":"2024-03-28T13:45:14.262438Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_df_from_dict(img_dict):\n    df = pd.DataFrame({k: v[0] for k,v in img_dict.items()}, index = ['Class']).T\n    return df\ndf_all = create_df_from_dict(img_dict)\ndf_all.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:46:16.185833Z","iopub.execute_input":"2024-03-28T13:46:16.187261Z","iopub.status.idle":"2024-03-28T13:46:16.243958Z","shell.execute_reply.started":"2024-03-28T13:46:16.187175Z","shell.execute_reply":"2024-03-28T13:46:16.242407Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                    Class\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0302_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0114_MR1_mpr-1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0150_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0253_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0349_MR1_mpr-4</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming df is your DataFrame and 'class' is your target feature\nX = df_all.drop('Class', axis=1)\ny = df_all['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:46:16.391455Z","iopub.execute_input":"2024-03-28T13:46:16.392029Z","iopub.status.idle":"2024-03-28T13:46:16.413798Z","shell.execute_reply.started":"2024-03-28T13:46:16.391974Z","shell.execute_reply":"2024-03-28T13:46:16.412521Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_train(img_dict, y_train, y_test):\n    X_train_n = []\n    y_train_n = []\n    X_test_n = []\n    y_test_n = []\n    for ind in y_test.index:\n        paths = img_dict[ind]\n        imgs = []\n        for j in range(1,len(paths)):\n            img = Image.open(paths[j]).resize((128,128))\n            img = np.array(img)\n            imgs.append(img)\n            #y_test_n.append(y_test[ind])\n        X_test_n.append(imgs)\n        \n    print('first part done')\n    for ind in y_train.index:\n        paths = img_dict[ind]\n        imgs = []\n        for j in range(1,len(paths)):\n            img = Image.open(paths[j]).resize((128,128))\n            img = np.array(img)\n            imgs.append(img)\n            #y_train_n.append(y_train[ind])\n        X_train_n.append(imgs)\n    \n    \n    return X_train_n, X_test_n, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:46:42.947592Z","iopub.execute_input":"2024-03-28T13:46:42.947976Z","iopub.status.idle":"2024-03-28T13:46:42.967704Z","shell.execute_reply.started":"2024-03-28T13:46:42.947946Z","shell.execute_reply":"2024-03-28T13:46:42.965920Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = create_test_train(img_dict, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:47:47.787181Z","iopub.execute_input":"2024-03-28T13:47:47.787578Z","iopub.status.idle":"2024-03-28T14:03:51.761345Z","shell.execute_reply.started":"2024-03-28T13:47:47.787549Z","shell.execute_reply":"2024-03-28T14:03:51.760038Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"first part done\n","output_type":"stream"}]},{"cell_type":"code","source":"del path0, path1, path2, path3, X, y\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:03:51.763661Z","iopub.execute_input":"2024-03-28T14:03:51.764655Z","iopub.status.idle":"2024-03-28T14:03:51.773132Z","shell.execute_reply.started":"2024-03-28T14:03:51.764611Z","shell.execute_reply":"2024-03-28T14:03:51.771757Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nx_train = torch.from_numpy(np.array(X_train))\ny_train = torch.from_numpy(np.array(y_train))\nx_test = torch.from_numpy(np.array(X_test))\ny_test = torch.from_numpy(np.array(y_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:03:51.775387Z","iopub.execute_input":"2024-03-28T14:03:51.775737Z","iopub.status.idle":"2024-03-28T14:03:58.380974Z","shell.execute_reply.started":"2024-03-28T14:03:51.775709Z","shell.execute_reply":"2024-03-28T14:03:58.378684Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:03:58.385036Z","iopub.execute_input":"2024-03-28T14:03:58.385829Z","iopub.status.idle":"2024-03-28T14:03:58.394077Z","shell.execute_reply.started":"2024-03-28T14:03:58.385782Z","shell.execute_reply":"2024-03-28T14:03:58.392592Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([1133, 61, 128, 128, 3])"},"metadata":{}}]},{"cell_type":"code","source":"del X_train, X_test, df_all, img_dict","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:03:58.395408Z","iopub.execute_input":"2024-03-28T14:03:58.395770Z","iopub.status.idle":"2024-03-28T14:03:58.447233Z","shell.execute_reply.started":"2024-03-28T14:03:58.395740Z","shell.execute_reply":"2024-03-28T14:03:58.445884Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print('Time to run the network')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:03:58.460969Z","iopub.execute_input":"2024-03-28T14:03:58.461701Z","iopub.status.idle":"2024-03-28T14:03:58.469709Z","shell.execute_reply.started":"2024-03-28T14:03:58.461639Z","shell.execute_reply":"2024-03-28T14:03:58.468111Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Time to run the network\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-28T09:59:37.482138Z","iopub.execute_input":"2024-03-28T09:59:37.482438Z","iopub.status.idle":"2024-03-28T09:59:37.527804Z","shell.execute_reply.started":"2024-03-28T09:59:37.482416Z","shell.execute_reply":"2024-03-28T09:59:37.526922Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-28T09:59:37.537912Z","iopub.execute_input":"2024-03-28T09:59:37.538171Z","iopub.status.idle":"2024-03-28T09:59:37.547084Z","shell.execute_reply.started":"2024-03-28T09:59:37.538149Z","shell.execute_reply":"2024-03-28T09:59:37.546240Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-28T09:59:37.547966Z","iopub.execute_input":"2024-03-28T09:59:37.548220Z","iopub.status.idle":"2024-03-28T09:59:37.562317Z","shell.execute_reply.started":"2024-03-28T09:59:37.548177Z","shell.execute_reply":"2024-03-28T09:59:37.561479Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = torch.device(\"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:06:12.523991Z","iopub.execute_input":"2024-03-28T14:06:12.524820Z","iopub.status.idle":"2024-03-28T14:06:12.532066Z","shell.execute_reply.started":"2024-03-28T14:06:12.524758Z","shell.execute_reply":"2024-03-28T14:06:12.530761Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Define your custom dataset\nclass MyDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Assuming x_train, y_train, x_test, y_test are your data\ntrain_dataset = MyDataset(x_train, y_train)\ntest_dataset = MyDataset(x_test, y_test)\n\n# Create DataLoaders for training and testing\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Define the neural network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Change 1 to 3\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.rnn = nn.GRU(64*32*32, 256, num_layers=2, batch_first=True)\n        self.fc = nn.Linear(256, 4)\n\n    def forward(self, x):\n        batch_size, timesteps, C, H, W = x.size()\n        x = x.view(batch_size * timesteps, C, H, W)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.reshape(batch_size, timesteps, -1)  # Use reshape instead of view\n        x, _ = self.rnn(x)\n        x = x[:, -1, :]\n        x = torch.sigmoid(self.fc(x))\n        return x\n\n\n\n# Instantiate the network\nmodel = Net().to(device)\n\n# Define the loss function and the optimizer\ncriterion = nn.BCELoss()\nlr = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr = lr)\n\n\n# Initialize empty lists to store losses\ntrain_losses = []\ntest_losses = []\n\n# Training loop\nfor epoch in range(10):  # loop over the dataset multiple times\n    running_loss = 0.0\n    model.train()  # Set the model to training mode\n    for i, data in enumerate(train_dataloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        inputs = inputs.permute(0,1,4,2,3)\n        # Convert labels to one-hot encoding\n        labels_onehot = torch.zeros(labels.size(0), 4, device = device)\n        labels_onehot.scatter_(1, labels.view(-1,1), 1)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels_onehot)\n        loss.backward()\n        optimizer.step()\n        # Compute training loss\n        train_losses.append(loss.item())\n        # print statistics\n        running_loss += loss.item()\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader)}')\n\n    # Compute test loss\n    #test_loss = 0.0\n    #model.eval()  # Set the model to evaluation mode\n    #with torch.no_grad():  # Do not compute gradients for these operations\n    #    for i, data in enumerate(test_dataloader, 0):\n    #        inputs, labels = data\n    #        inputs = inputs.float()\n    #        inputs = inputs.permute(0,1,4,2,3)\n    #        labels_onehot = torch.zeros(labels.size(0), 4,device = device)\n    #        labels_onehot.scatter_(1, labels.view(-1,1), 1)\n    #        outputs = model(inputs)\n    #        loss = criterion(outputs, labels_onehot)\n    #        test_loss += loss.item()\n    #        test_losses.append(test_loss / len(test_dataloader))  # Store the test loss\n    #print(f'Epoch {epoch+1}, test loss: {test_loss/len(test_dataloader)}')\n\nprint('Finished Training')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:06:32.616461Z","iopub.execute_input":"2024-03-28T14:06:32.616857Z","iopub.status.idle":"2024-03-28T14:06:41.902858Z","shell.execute_reply.started":"2024-03-28T14:06:32.616825Z","shell.execute_reply":"2024-03-28T14:06:41.901107Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_onehot)\n\u001b[1;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_size, timesteps, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m timesteps, C, H, W)\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, timesteps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Use reshape instead of view\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-28T13:32:09.699992Z","iopub.execute_input":"2024-03-28T13:32:09.700369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\n# Define your custom dataset\nclass MyDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\n# Assuming x_train, y_train, x_test, y_test are your data\ntrain_dataset = MyDataset(x_train, y_train)\ntest_dataset = MyDataset(x_test, y_test)\n\n# Create DataLoaders for training and testing\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Define the neural network with dropout\nclass Net(nn.Module):\n    def __init__(self, dropout_prob=0.5):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n        self.rnn = nn.GRU(64*32*32, 256, num_layers=2, batch_first=True)\n        self.fc = nn.Linear(256, 4)\n\n    def forward(self, x):\n        batch_size, timesteps, C, H, W = x.size()\n        x = x.view(batch_size * timesteps, C, H, W)\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.reshape(batch_size, timesteps, -1)\n        x, _ = self.rnn(x)\n        x = x[:, -1, :]\n        x = torch.sigmoid(self.fc(x))\n        return x\n\n# Instantiate the network and move it to the device\nmodel = Net().to(device)\n\n# Define the loss function and the optimizer\ncriterion = nn.BCELoss()\nlr = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n# Initialize empty lists to store losses\ntrain_losses = []\n\n# Training loop\nfor epoch in range(10):\n    running_loss = 0.0\n    model.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs = inputs.float()\n        inputs = inputs.permute(0, 1, 4, 2, 3)\n        labels_onehot = torch.zeros(labels.size(0), 4, device=device)\n        labels_onehot.scatter_(1, labels.view(-1, 1), 1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels_onehot)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Compute training loss\n        train_losses.append(loss.item())\n    print(f'Epoch {epoch+1}, training loss: {running_loss/len(train_dataloader)}')\n\n    \nprint('Finished Training')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-28T14:12:33.991756Z","iopub.execute_input":"2024-03-28T14:12:33.992330Z","iopub.status.idle":"2024-03-28T18:11:41.630940Z","shell.execute_reply.started":"2024-03-28T14:12:33.992292Z","shell.execute_reply":"2024-03-28T18:11:41.629827Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1, training loss: 0.35152021133237416\nEpoch 2, training loss: 0.3066164462102784\nEpoch 3, training loss: 0.3059644007848369\nEpoch 4, training loss: 0.30507295206189156\nEpoch 5, training loss: 0.3052570815715525\nEpoch 6, training loss: 0.30500855586595005\nEpoch 7, training loss: 0.3030266563097636\nEpoch 8, training loss: 0.30570755360854995\nEpoch 9, training loss: 0.30465279809302753\nEpoch 10, training loss: 0.3049928872949547\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, classification_report\n\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    raw_outputs = model(x_test.float().permute(0,1,4,2,3))  # Raw output (logits) from the model\n    predicted_probs = torch.sigmoid(raw_outputs)  # Apply sigmoid to get probabilities\n\n\n# Assuming you have predictions (predicted_probs) and ground truth labels (y_test)\nthreshold = 0.5  # Set your desired threshold for converting probabilities to binary labels\npredicted_labels = (predicted_probs > threshold).astype(int)\n\n# Compute multilabel confusion matrix\nmcm = multilabel_confusion_matrix(y_test, predicted_labels)\n\n# Print confusion matrix for each class\nfor i, label in enumerate([\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"]):\n    print(f\"Confusion matrix for {label}:\\n{mcm[i]}\")\n\n# Compute classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, predicted_labels))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}