{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport os\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"5e3bbedd-c862-4060-9174-8f328e43f721","_cell_guid":"28fc7583-37c8-4c12-a0d5-77a2e08b6c48","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-23T19:44:06.887633Z","iopub.execute_input":"2024-03-23T19:44:06.888094Z","iopub.status.idle":"2024-03-23T19:44:06.896160Z","shell.execute_reply.started":"2024-03-23T19:44:06.888061Z","shell.execute_reply":"2024-03-23T19:44:06.894629Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"path1 = []\npath2 = []\npath3 = []\npath0 = []\nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Non Demented'):\n    for filename in filenames:\n        path0.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Mild Dementia'):\n    for filename in filenames:\n        path1.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Moderate Dementia'):\n    for filename in filenames:\n        path3.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/imagesoasis/Data/Very mild Dementia'):\n    for filename in filenames:\n        path2.append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:08.438886Z","iopub.execute_input":"2024-03-23T19:44:08.439275Z","iopub.status.idle":"2024-03-23T19:44:59.378639Z","shell.execute_reply.started":"2024-03-23T19:44:08.439245Z","shell.execute_reply":"2024-03-23T19:44:59.377409Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dict(file_names):\n    img_dict = {}\n    for name in file_names:\n        ind = name[:-8]\n        if ind in img_dict:\n            img_dict[ind].append(name)\n        else:\n            dem = name[31:34]\n            if dem=='Non':\n                img_dict[ind] = [0]\n            elif dem == 'Mil':\n                img_dict[ind] = [1]\n            elif dem == 'Ver':\n                img_dict[ind] = [2]\n            else:\n                img_dict[ind] = [3]\n            img_dict[ind].append(name)\n    return img_dict\nimg_dict = create_dict(path0 + path1 + path2 + path3)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:59.380705Z","iopub.execute_input":"2024-03-23T19:44:59.381026Z","iopub.status.idle":"2024-03-23T19:44:59.455297Z","shell.execute_reply.started":"2024-03-23T19:44:59.381000Z","shell.execute_reply":"2024-03-23T19:44:59.454001Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def create_df_from_dict(img_dict):\n    df = pd.DataFrame({k: v[0] for k,v in img_dict.items()}, index = ['Class']).T\n    return df\ndf_all = create_df_from_dict(img_dict)\ndf_all.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:59.456703Z","iopub.execute_input":"2024-03-23T19:44:59.457388Z","iopub.status.idle":"2024-03-23T19:44:59.504006Z","shell.execute_reply.started":"2024-03-23T19:44:59.457356Z","shell.execute_reply":"2024-03-23T19:44:59.502630Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                                                    Class\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0\n/kaggle/input/imagesoasis/Data/Non Demented/OAS...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0302_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0114_MR1_mpr-1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0150_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0253_MR1_mpr-3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>/kaggle/input/imagesoasis/Data/Non Demented/OAS1_0349_MR1_mpr-4</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming df is your DataFrame and 'class' is your target feature\nX = df_all.drop('Class', axis=1)\ny = df_all['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:59.506372Z","iopub.execute_input":"2024-03-23T19:44:59.506754Z","iopub.status.idle":"2024-03-23T19:44:59.519981Z","shell.execute_reply.started":"2024-03-23T19:44:59.506724Z","shell.execute_reply":"2024-03-23T19:44:59.518512Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_train(img_dict, y_train, y_test):\n    X_train_n = []\n    y_train_n = []\n    X_test_n = []\n    y_test_n = []\n    for ind in y_test.index:\n        paths = img_dict[ind]\n        imgs = []\n        for j in range(1,len(paths)):\n            img = Image.open(paths[j]).resize((128,128))\n            img = np.array(img)\n            imgs.append(img)\n            y_test_n.append(y_test[ind])\n        X_test_n.append(imgs)\n        \n    print('first part done')\n    for ind in y_train.index:\n        paths = img_dict[ind]\n        imgs = []\n        for j in range(1,len(paths)):\n            img = Image.open(paths[j]).resize((128,128))\n            img = np.array(img)\n            imgs.append(img)\n            y_train_n.append(y_train[ind])\n        X_train_n.append(imgs)\n    \n    \n    return X_train_n, X_test_n, y_train_n, y_test_n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:59.522707Z","iopub.execute_input":"2024-03-23T19:44:59.523593Z","iopub.status.idle":"2024-03-23T19:44:59.535291Z","shell.execute_reply.started":"2024-03-23T19:44:59.523550Z","shell.execute_reply":"2024-03-23T19:44:59.533903Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = create_test_train(img_dict, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:44:59.538401Z","iopub.execute_input":"2024-03-23T19:44:59.539648Z","iopub.status.idle":"2024-03-23T19:49:54.183068Z","shell.execute_reply.started":"2024-03-23T19:44:59.539595Z","shell.execute_reply":"2024-03-23T19:49:54.181332Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"first part done\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n# Assuming each image is 128x128x3\ninput_shape = (128, 128, 3)\n\n# Define the CNN model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.flatten = nn.Flatten()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.flatten(x)\n        return x\n\n# Create 61 CNN models for the 61 input images\ncnn_models = [CNNModel() for _ in range(61)]\n\n# Create 61 inputs for the 61 input images\ninputs = [torch.randn(input_shape).permute(2,0,1) for _ in range(61)]\n\n# Extract features from each input using the corresponding CNN model\nfeatures = [model(inp) for model, inp in zip(cnn_models, inputs)]\n\n# Concatenate all features together\nconcatenated = torch.cat(features, dim=1)\n\n# Add a dense layer for final prediction\noutput_layer = nn.Linear(concatenated.size(1), 4)\noutput = output_layer(concatenated)\n\n# Create the model\nmodel = nn.ModuleList([*cnn_models, output_layer])\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:40:40.695353Z","iopub.execute_input":"2024-03-23T20:40:40.695688Z","iopub.status.idle":"2024-03-23T20:40:43.517497Z","shell.execute_reply.started":"2024-03-23T20:40:40.695666Z","shell.execute_reply":"2024-03-23T20:40:43.516643Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"ModuleList(\n  (0-60): 61 x CNNModel(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (61): Linear(in_features=242109, out_features=4, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for id_images, label in zip(X_train, y_train):\n    outputs = model(*id_images)\n    loss = nn.CrossEntropyLoss(outputs.unsqueeze(0), label)\n\n# Switch to evaluation mode\nmodel.eval()\n\n# No need to track gradients for testing\nwith torch.no_grad():\n    correct_predictions = 0\n    total_predictions = 0\n\n    for id_images, label in zip(X_test, y_test):\n        outputs = model(*id_images)\n        _, predicted = torch.max(outputs.data, 1)\n        total_predictions += label.size(0)\n        correct_predictions += (predicted == label).sum().item()\n\n    print('Test Accuracy: %d %%' % (100 * correct_predictions / total_predictions))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}